{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 7: Policy Gradient\n",
    "\n",
    "### Exercise 7.1: REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment features the Cartpole domain which tasks the agent with balancing a pole affixed to a movable cart. The agent employs two discrete actions which apply force to the cart. Episodes provide +1 reward for each step in which the pole has not fallen over, up to a maximum of 200 steps. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "* Understand and implement REINFORCE loss function: $\\nabla_\\theta J(\\theta)=\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R$\n",
    "* Implement psuedo-labels needed to train the policy network\n",
    "* Verify that your REINFORCE implementation solves Cartpole\n",
    "\n",
    "## Success Criterion\n",
    "When correctly implemented, the REINFORCE algorithm should be able to balance the poll for 200 steps within a few hundred episodes of learning. However, it's recommended to run the algorithm several times, as even a correct algorithm may fail to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "    \n",
    "from lib.running_variance import RunningVariance\n",
    "from itertools import count\n",
    "from lib import plotting\n",
    "\n",
    "np.random.seed(123)\n",
    "C.cntk_py.set_fixed_random_seed(123)\n",
    "C.cntk_py.force_deterministic_algorithms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished with imports we need to create the CartPole environment. (See https://gym.openai.com/envs/CartPole-v0/) for more details. Additionally, we'll save the size of the state and action spaces, and define the number of hidden units in our network as well as how often (in episodes) the Reinforce update is applied. These parameters don't need to be changed, but you can try varying the hidden_size and update_frequency and see how learning is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state_dim = env.observation_space.shape[0] # Dimension of state space\n",
    "action_count = env.action_space.n # Number of actions\n",
    "hidden_size = 128 # Number of hidden units\n",
    "update_frequency = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy network maps an observation to a probability of taking action 0 or 1.\n",
    "observations = C.sequence.input_variable(state_dim, np.float32, name=\"obs\")\n",
    "W1 = C.parameter(shape=(state_dim, hidden_size), init=C.glorot_uniform(), name=\"W1\")\n",
    "b1 = C.parameter(shape=hidden_size, name=\"b1\")\n",
    "layer1 = C.relu(C.times(observations, W1) + b1)\n",
    "W2 = C.parameter(shape=(hidden_size, action_count), init=C.glorot_uniform(), name=\"W2\")\n",
    "b2 = C.parameter(shape=action_count, name=\"b2\")\n",
    "layer2 = C.times(layer1, W2) + b2\n",
    "output = C.sigmoid(layer2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must define the loss function for training the policy network. \n",
    "\n",
    "- Recall that the desired loss function is: $\\frac{1}{m}\\sum_1^m \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R$. \n",
    "\n",
    "- Label is a variable corresponding to $a_t$, the action the policy selected. \n",
    "\n",
    "- output is the policy network that maps an observation to a probability of taking an action.\n",
    "\n",
    "- And return_weight is a scalar that will cointain the return $R$.\n",
    "\n",
    "The current loss function is incorrect and will need to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label will tell the network what action it should have taken.\n",
    "label = C.sequence.input_variable(1, np.float32, name=\"label\")\n",
    "# return_weight is a scalar containing the discounted return. It will scale the PG loss.\n",
    "return_weight = C.sequence.input_variable(1, np.float32, name=\"weight\")\n",
    "\n",
    "# TODO 1\n",
    "# Modify loss function to implement policy gradient loss.\n",
    "# The loss should encourage the network's output to become more like the provided label, \n",
    "# when the Return is positive and less like the label when the Return is negative.\n",
    "# PG_Loss = mean( log(probability(a|s)) * R(s))\n",
    "# Hint: C.log() and C.square() may come in handy. \n",
    "loss = -C.reduce_mean(label + output + return_weight, axis=0, name='loss') ## modify this loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build the optimizer that contains hyperparameters for training the policy network. Also we create a buffer which will accumulate gradients throughout update_frequency episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the optimizer\n",
    "lr_schedule = C.learning_rate_schedule(lr=0.1, unit=C.UnitType.sample) \n",
    "m_schedule = C.momentum_schedule(0.99)\n",
    "vm_schedule = C.momentum_schedule(0.999)\n",
    "optimizer = C.adam([W1, W2], lr_schedule, momentum=m_schedule, variance_momentum=vm_schedule)\n",
    "\n",
    "# Create a buffer to manually accumulate gradients\n",
    "gradBuffer = dict((var.name, np.zeros(shape=var.shape)) for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to discount rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.999):\n",
    "    \"\"\"Take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the main training loop. The only part that needs to be modified is the code for choosing a psuedo-label. Remember this pseudo-label should encourage the agent to increase the probability of the action it selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "running_variance = RunningVariance()\n",
    "reward_sum = 0\n",
    "\n",
    "max_number_of_episodes = 500\n",
    "\n",
    "stats = plotting.EpisodeStats(\n",
    "    episode_lengths=np.zeros(max_number_of_episodes),\n",
    "    episode_rewards=np.zeros(max_number_of_episodes),\n",
    "    episode_running_variance=np.zeros(max_number_of_episodes))   \n",
    "    \n",
    "for episode_number in range(max_number_of_episodes):\n",
    "    states, rewards, labels = [],[],[]\n",
    "    done = False\n",
    "    \n",
    "    observation = env.reset()\n",
    "    t = 1\n",
    "    while not done:\n",
    "        \n",
    "        state = np.reshape(observation, [1, state_dim]).astype(np.float32)\n",
    "        states.append(state)\n",
    "\n",
    "        # Run the policy network and get an action to take.\n",
    "        prob = output.eval(arguments={observations: state})[0][0][0]\n",
    "        # Sample from the bernoulli output distribution to get a discrete action\n",
    "        action = 1 if np.random.uniform() < prob else 0\n",
    "\n",
    "        # TODO 2\n",
    "        # Modify saved labels to encourage the network to increase\n",
    "        # the probability of the chosen action. This label will be used\n",
    "        # in the loss function above.\n",
    "        \n",
    "        y = 1 # modify 1 to create a \"fake label\" or pseudo label\n",
    "        labels.append(y)\n",
    "    \n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Uncomment this line to render the environment\n",
    "        # env.render()\n",
    "\n",
    "        reward_sum += float(reward)\n",
    "\n",
    "        # Record reward (has to be done after we call step() to get reward for previous action)\n",
    "        rewards.append(float(reward))\n",
    "                    \n",
    "        stats.episode_rewards[episode_number] += reward\n",
    "        stats.episode_lengths[episode_number] = t\n",
    "        \n",
    "        t += 1\n",
    "\n",
    "    # Stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(states)\n",
    "    epl = np.vstack(labels).astype(np.float32)\n",
    "    epr = np.vstack(rewards).astype(np.float32)\n",
    "\n",
    "    # Compute the discounted reward backwards through time.\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "\n",
    "    # Keep a running estimate over the variance of of the discounted rewards\n",
    "    for r in discounted_epr:    \n",
    "        running_variance.add(r[0])\n",
    "\n",
    "    # Forward pass\n",
    "    arguments = {observations: epx, label: epl, return_weight: discounted_epr}\n",
    "    state, outputs_map = loss.forward(arguments, outputs=loss.outputs,\n",
    "                                      keep_for_backward=loss.outputs)\n",
    "\n",
    "    # Backward pass\n",
    "    root_gradients = {v: np.ones_like(o) for v, o in outputs_map.items()}\n",
    "    vargrads_map = loss.backward(state, root_gradients, variables=set([W1, W2]))\n",
    "\n",
    "    for var, grad in vargrads_map.items():\n",
    "        gradBuffer[var.name] += grad\n",
    "\n",
    "    # Only update every 20 episodes to reduce noise\n",
    "    if episode_number % update_frequency == 0:\n",
    "        grads = {W1: gradBuffer['W1'].astype(np.float32),\n",
    "                 W2: gradBuffer['W2'].astype(np.float32)}\n",
    "        updated = optimizer.update(grads, update_frequency)\n",
    "\n",
    "        # reset the gradBuffer\n",
    "        gradBuffer = dict((var.name, np.zeros(shape=var.shape))\n",
    "                          for var in loss.parameters if var.name in ['W1', 'W2', 'b1', 'b2'])\n",
    "\n",
    "        print('Episode: %d. Average reward for episode %f. Variance %f' % (episode_number, reward_sum / update_frequency, running_variance.get_variance()))\n",
    "\n",
    "        reward_sum = 0\n",
    "\n",
    "    stats.episode_running_variance[episode_number] = running_variance.get_variance()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_pgresults(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
