{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 6: Function Approximation\n",
    "\n",
    "### Exercise 6.1: Q-Learning Agent with Linear Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "\n",
    "from lib.envs.simple_rooms import SimpleRoomsEnv\n",
    "from lib.simulation import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):  \n",
    "        \n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "\n",
    "    def act(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QLearningFAAgent(Agent):\n",
    "    def __init__(self, actions, obs_size, epsilon=0.01, alpha=0.5, gamma=1):\n",
    "        super(QLearningFAAgent, self).__init__(actions)\n",
    "        \n",
    "        ## TODO 1\n",
    "        ## Initialize thetas here\n",
    "        ## In addition, initialize the value of epsilon, alpha and gamma\n",
    "        \n",
    "    def featureExtractor(self, state, action):\n",
    "        feature = None\n",
    "        \n",
    "        actionindex = np.zeros(self.num_actions, dtype=np.int)\n",
    "        actionindex[action] = 1\n",
    "        feature = np.concatenate([actionindex[i] * state for i in self.actions])\n",
    "        return feature    \n",
    "            \n",
    "    def act(self, state):\n",
    "        ## epsilon greedy policy \n",
    "        if np.random.random() < self.epsilon:\n",
    "            i = np.random.randint(0,len(self.actions))\n",
    "        else:\n",
    "            #q = [np.sum(self.theta.transpose() * self.featureExtractor(state, a)) for a in self.actions]\n",
    "                    \n",
    "            ## TODO 2\n",
    "            q = 0 # replace 0 with the correct calculation here\n",
    "            \n",
    "            if q.count(max(q)) > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == max(q)]\n",
    "                i = np.random.choice(best)\n",
    "            else:\n",
    "                i = q.index(max(q))\n",
    "            \n",
    "        action = self.actions[i]\n",
    "        return action           \n",
    "            \n",
    "    def learn(self, state1, action1, reward, state2, done):\n",
    "        \n",
    "        \"\"\"\n",
    "        Q-learning with FA \n",
    "        theta <- theta + alpha * td_delta * f(s,a)\n",
    "        where \n",
    "        td_delta = reward + gamma * max(Q(s') - Q(s,a))\n",
    "        Q(s,a) = thetas * f(s,a)\n",
    "        max(Q(s')) = max( [ thetas * f(s'a) for a in all actions] )\n",
    "        \n",
    "        \"\"\"\n",
    "        ## TODO 3\n",
    "        ## Implement the q-learning update here\n",
    "        \n",
    "        maxqnew = 0 # replace 0 with the correct calculation\n",
    "        oldv = 0 # replace 0 with the correct calculation\n",
    "        \n",
    "        td_target = reward + self.gamma * maxqnew\n",
    "        td_delta = td_target - oldv\n",
    "        self.theta += self.alpha * 0 # replace 0 with the correct calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactive = True\n",
    "%matplotlib nbagg\n",
    "env = SimpleRoomsEnv()\n",
    "agent = QLearningFAAgent(range(env.action_space.n),16)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(10, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interactive = False\n",
    "%matplotlib inline\n",
    "env = SimpleRoomsEnv()\n",
    "agent = QLearningFAAgent(range(env.action_space.n),16)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(50, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
