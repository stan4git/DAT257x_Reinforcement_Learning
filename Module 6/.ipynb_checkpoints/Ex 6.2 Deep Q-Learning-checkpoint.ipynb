{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DAT257x: Reinforcement Learning Explained\n",
    "\n",
    "## Lab 6: Function Approximation\n",
    "\n",
    "### Exercise 6.2: Deep Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "\n",
    "from lib.envs.simple_rooms import SimpleRoomsEnv\n",
    "from lib.simulation import Experiment\n",
    "\n",
    "try:\n",
    "    import chainer\n",
    "except ImportError as e:\n",
    "    !pip install chainer\n",
    "    import chainer\n",
    "    \n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import initializers, optimizers, Chain, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):  \n",
    "        \n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.num_actions = len(actions)\n",
    "\n",
    "    def act(self, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQLearningAgent(Agent):\n",
    "    \"\"\"Q-Learning agent with function approximation.\"\"\"\n",
    "\n",
    "    def __init__(self, actions, obs_size, **kwargs):\n",
    "        super(DQLearningAgent, self).__init__(actions)\n",
    "\n",
    "        self.obs_size = obs_size\n",
    "        \n",
    "        self.step_counter = 0\n",
    "        self.epsilon = kwargs.get('epsilon', .01)       \n",
    "        # if epsilon set to 1, it will be decayed over time\n",
    "        if self.epsilon == 1:\n",
    "            self.epsilon_decay = True\n",
    "        else:\n",
    "            self.epsilon_decay = False\n",
    "            \n",
    "        self.gamma = kwargs.get('gamma', .99)\n",
    "        \n",
    "        self.minibatch_size = kwargs.get('minibatch_size', 32)\n",
    "        self.epoch_length = kwargs.get('epoch_length', 100)\n",
    "        self.tau = kwargs.get('tau', .001)\n",
    "        self.model_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
    "        self.target_network = QNetwork(self.obs_size, self.num_actions, kwargs.get('nhidden', 512))\n",
    "        self.target_network.copyparams(self.model_network)\n",
    "        \n",
    "        self.optimizer = self.init_optimizer(self.model_network, kwargs.get('learning_rate', .5))\n",
    "\n",
    "        self.memory = ReplayMemory(self.obs_size, kwargs.get('mem_size', 10000))\n",
    "        \n",
    "        self.current_loss = .0\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            i = np.random.randint(0,len(self.actions))\n",
    "        else: \n",
    "            Q = self.model_network(Variable(state.reshape(1, state.shape[0])))\n",
    "            i = Q.data.argmax()\n",
    "            \n",
    "        self.step_counter += 1 \n",
    "        # decay epsilon after each epoch\n",
    "        if self.epsilon_decay:\n",
    "            if self.step_counter % self.epoch_length == 0:\n",
    "                self.epsilon = max(.01, self.epsilon * .95)\n",
    "        \n",
    "        action = self.actions[i]        \n",
    "        return action     \n",
    "    \n",
    "    def learn(self, state1, action1, reward, state2, done):\n",
    "        self.memory.observe(state1, action1, reward, done)\n",
    "        # start training after 1 epoch\n",
    "        if self.step_counter > self.epoch_length:\n",
    "            self.current_loss = self.update_model()\n",
    "\n",
    "    def init_optimizer(self, model, learning_rate):\n",
    "\n",
    "        optimizer = optimizers.SGD(learning_rate)\n",
    "        # optimizer = optimizers.Adam(alpha=learning_rate)\n",
    "        # optimizer = optimizers.AdaGrad(learning_rate)\n",
    "        # optimizer = optimizers.RMSpropGraves(learning_rate, 0.95, self.momentum, 1e-2)\n",
    "\n",
    "        optimizer.setup(model)\n",
    "        return optimizer\n",
    "    \n",
    "    def update_model(self):\n",
    "        (s, action, reward, s_next, is_terminal) = self.memory.sample_minibatch(self.minibatch_size)\n",
    "\n",
    "        # compute Q targets (max_a' Q_hat(s_next, a'))\n",
    "        Q_hat = self.target_network(s_next)\n",
    "        Q_hat_max = F.max(Q_hat, axis=1, keepdims=True)\n",
    "        y = (1-is_terminal)*self.gamma*Q_hat_max + reward\n",
    "\n",
    "        # compute Q(s, action)\n",
    "        Q = self.model_network(s)\n",
    "        Q_subset = F.reshape(F.select_item(Q, action), (self.minibatch_size, 1))\n",
    "\n",
    "        # compute Huber loss\n",
    "        error = y - Q_subset\n",
    "        loss_clipped = abs(error) * (abs(error.data) > 1) + (error**2) * (abs(error.data) <= 1)\n",
    "        loss = F.sum(loss_clipped) / self.minibatch_size\n",
    "\n",
    "        # perform model update\n",
    "        self.model_network.zerograds() ## zero out the accumulated gradients in all network parameters\n",
    "        loss.backward()\n",
    "        self.optimizer.update()\n",
    "\n",
    "        # target network tracks the model\n",
    "        for dst, src in zip(self.target_network.params(), self.model_network.params()):\n",
    "            dst.data = self.tau * src.data + (1 - self.tau) * dst.data\n",
    "\n",
    "        return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNetwork(Chain):\n",
    "    \"\"\"The neural network architecture as a Chainer Chain - here: single hidden layer\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size, num_actions, nhidden):\n",
    "        \"\"\"Initialize weights\"\"\"\n",
    "        # use LeCunUniform weight initialization for weights\n",
    "        self.initializer = initializers.LeCunUniform()\n",
    "        self.bias_initializer = initializers.Uniform(1e-4)\n",
    "\n",
    "        super(QNetwork, self).__init__(\n",
    "            feature_layer = L.Linear(obs_size, nhidden,\n",
    "                                initialW = self.initializer,\n",
    "                                initial_bias = self.bias_initializer),\n",
    "            action_values = L.Linear(nhidden, num_actions, \n",
    "                                initialW=self.initializer,\n",
    "                                initial_bias = self.bias_initializer)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"implements forward pass\"\"\"\n",
    "        h = F.relu(self.feature_layer(x))\n",
    "        return self.action_values(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Implements basic replay memory\"\"\"\n",
    "\n",
    "    def __init__(self, observation_size, max_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.num_observed = 0\n",
    "        self.max_size = max_size\n",
    "        self.samples = {\n",
    "                 'obs'      : np.zeros(self.max_size * 1 * self.observation_size,\n",
    "                                       dtype=np.float32).reshape(self.max_size, 1, self.observation_size),\n",
    "                 'action'   : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "                 'reward'   : np.zeros(self.max_size * 1).reshape(self.max_size, 1),\n",
    "                 'terminal' : np.zeros(self.max_size * 1, dtype=np.int16).reshape(self.max_size, 1),\n",
    "               }\n",
    "\n",
    "    def observe(self, state, action, reward, done):\n",
    "        index = self.num_observed % self.max_size\n",
    "        self.samples['obs'][index, :] = state\n",
    "        self.samples['action'][index, :] = action\n",
    "        self.samples['reward'][index, :] = reward\n",
    "        self.samples['terminal'][index, :] = done\n",
    "\n",
    "        self.num_observed += 1\n",
    "\n",
    "    def sample_minibatch(self, minibatch_size):\n",
    "        max_index = min(self.num_observed, self.max_size) - 1\n",
    "        sampled_indices = np.random.randint(max_index, size=minibatch_size)\n",
    "\n",
    "        s      = Variable(np.asarray(self.samples['obs'][sampled_indices, :], dtype=np.float32))\n",
    "        s_next = Variable(np.asarray(self.samples['obs'][sampled_indices+1, :], dtype=np.float32))\n",
    "\n",
    "        a      = Variable(self.samples['action'][sampled_indices].reshape(minibatch_size))\n",
    "        r      = self.samples['reward'][sampled_indices].reshape((minibatch_size, 1))\n",
    "        done   = self.samples['terminal'][sampled_indices].reshape((minibatch_size, 1))\n",
    "\n",
    "        return (s, a, r, s_next, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = True\n",
    "%matplotlib nbagg\n",
    "env = SimpleRoomsEnv()\n",
    "agent = DQLearningAgent(range(env.action_space.n), obs_size=16)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(10, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = False\n",
    "%matplotlib inline\n",
    "env = SimpleRoomsEnv()\n",
    "agent = DQLearningAgent(range(env.action_space.n), obs_size=16)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(50, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = False\n",
    "%matplotlib inline\n",
    "env = SimpleRoomsEnv()\n",
    "agent = DQLearningAgent(range(env.action_space.n), obs_size=16)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(200, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive = False\n",
    "%matplotlib inline\n",
    "env = SimpleRoomsEnv()\n",
    "agent = DQLearningAgent(range(env.action_space.n), obs_size=16, epsilon=1)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_qlearning(200, interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
